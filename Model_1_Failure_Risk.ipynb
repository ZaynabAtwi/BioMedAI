{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Failure Risk\n",
        "\n"
      ],
      "metadata": {
        "id": "-O6EW1zsdb4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9ALskR2dUs0"
      },
      "outputs": [],
      "source": [
        "\"\"\"Failure risk modeling pipeline for Scania sensor data and MaintNet maintenance logs.\n",
        "\n",
        "This module implements a reproducible, step-by-step pipeline for tabular modeling\n",
        "that follows the specification in the project brief:\n",
        "\n",
        "* Merge Scania sensor data with MaintNet work-order statistics per unit.\n",
        "* Perform lag-based feature engineering and aggregate statistics.\n",
        "* Address class imbalance via either SMOTE or cost-sensitive learning.\n",
        "* Train a baseline logistic regression model and an ensemble gradient boosted model\n",
        "  (LightGBM preferred, CatBoost as a fallback) with optional calibration.\n",
        "* Provide utilities for model explainability with SHAP at both global and local levels.\n",
        "\n",
        "The code is organized to make it easy to run each stage individually while maintaining\n",
        "an end-to-end `run_training_pipeline` entry-point that wires everything together.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import pathlib\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "\n",
        "try:  # Optional dependency that may not be available in all environments.\n",
        "    import lightgbm as lgb\n",
        "except ModuleNotFoundError:  # pragma: no cover - optional dependency guard\n",
        "    lgb = None  # type: ignore\n",
        "\n",
        "try:  # Optional dependency that may not be available in all environments.\n",
        "    from catboost import CatBoostClassifier\n",
        "except ModuleNotFoundError:  # pragma: no cover - optional dependency guard\n",
        "    CatBoostClassifier = None  # type: ignore\n",
        "\n",
        "try:  # Optional dependency that may not be available in all environments.\n",
        "    import shap\n",
        "except ModuleNotFoundError:  # pragma: no cover - optional dependency guard\n",
        "    shap = None  # type: ignore\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except ModuleNotFoundError as exc:  # pragma: no cover - imbalanced-learn may be optional\n",
        "    raise RuntimeError(\n",
        "        \"The failure risk pipeline requires the `imbalanced-learn` package.\"\n",
        "    ) from exc\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "LOGGER.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FailureRiskConfig:\n",
        "    \"\"\"Configuration for the failure risk modeling pipeline.\"\"\"\n",
        "\n",
        "    scania_path: pathlib.Path\n",
        "    maintnet_path: pathlib.Path\n",
        "    target_column: str = \"failure_event\"\n",
        "    unit_id_column: str = \"unit_id\"\n",
        "    timestamp_column: str = \"timestamp\"\n",
        "    train_size: float = 0.7\n",
        "    validation_size: float = 0.15\n",
        "    random_state: int = 42\n",
        "    use_smote: bool = True\n",
        "    calibration_method: Optional[str] = \"isotonic\"\n",
        "    text_column: Optional[str] = \"technician_remarks\"\n",
        "    text_max_features: int = 100\n",
        "    categorical_cols: List[str] = field(default_factory=list)\n",
        "    numerical_cols: List[str] = field(default_factory=list)\n",
        "    lag_days: Iterable[int] = field(default_factory=lambda: (7, 30, 90))\n",
        "\n",
        "\n",
        "def _load_dataframe(path: pathlib.Path) -> pd.DataFrame:\n",
        "    \"\"\"Load a dataframe from CSV, Parquet, or Feather based on extension.\"\"\"\n",
        "\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
        "\n",
        "    LOGGER.info(\"Loading data from %s\", path)\n",
        "    if path.suffix.lower() == \".csv\":\n",
        "        return pd.read_csv(path)\n",
        "    if path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
        "        return pd.read_parquet(path)\n",
        "    if path.suffix.lower() in {\".feather\", \".ft\"}:\n",
        "        return pd.read_feather(path)\n",
        "\n",
        "    raise ValueError(f\"Unsupported file format: {path.suffix}\")\n",
        "\n",
        "\n",
        "def load_scania_data(config: FailureRiskConfig) -> pd.DataFrame:\n",
        "    \"\"\"Load Scania sensor data and ensure timestamps are parsed.\"\"\"\n",
        "\n",
        "    scania_df = _load_dataframe(config.scania_path)\n",
        "    if config.timestamp_column in scania_df.columns:\n",
        "        scania_df[config.timestamp_column] = pd.to_datetime(\n",
        "            scania_df[config.timestamp_column], errors=\"coerce\"\n",
        "        )\n",
        "    return scania_df\n",
        "\n",
        "\n",
        "def load_maintnet_data(config: FailureRiskConfig) -> pd.DataFrame:\n",
        "    \"\"\"Load MaintNet work-order statistics for the vehicle subset.\"\"\"\n",
        "\n",
        "    maintnet_df = _load_dataframe(config.maintnet_path)\n",
        "    if config.timestamp_column in maintnet_df.columns:\n",
        "        maintnet_df[config.timestamp_column] = pd.to_datetime(\n",
        "            maintnet_df[config.timestamp_column], errors=\"coerce\"\n",
        "        )\n",
        "    return maintnet_df\n",
        "\n",
        "\n",
        "def merge_datasets(\n",
        "    scania_df: pd.DataFrame,\n",
        "    maintnet_df: pd.DataFrame,\n",
        "    config: FailureRiskConfig,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Merge Scania sensor data with MaintNet work-order statistics by unit and time.\"\"\"\n",
        "\n",
        "    required_cols = {config.unit_id_column}\n",
        "    missing_in_scania = required_cols - set(scania_df.columns)\n",
        "    missing_in_maintnet = required_cols - set(maintnet_df.columns)\n",
        "    if missing_in_scania:\n",
        "        raise KeyError(f\"Missing columns in Scania data: {missing_in_scania}\")\n",
        "    if missing_in_maintnet:\n",
        "        raise KeyError(f\"Missing columns in MaintNet data: {missing_in_maintnet}\")\n",
        "\n",
        "    LOGGER.info(\"Merging Scania and MaintNet data on %s\", config.unit_id_column)\n",
        "    merged = pd.merge(\n",
        "        scania_df,\n",
        "        maintnet_df,\n",
        "        on=config.unit_id_column,\n",
        "        how=\"left\",\n",
        "        suffixes=(\"_scania\", \"_maintnet\"),\n",
        "    )\n",
        "\n",
        "    if config.timestamp_column in merged.columns:\n",
        "        merged = merged.sort_values(by=[config.unit_id_column, config.timestamp_column])\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "def _lag_feature_name(base: str, lag_days: int, agg: str) -> str:\n",
        "    return f\"{base}_lag_{lag_days}_days_{agg}\"\n",
        "\n",
        "\n",
        "def engineer_lag_features(\n",
        "    df: pd.DataFrame,\n",
        "    config: FailureRiskConfig,\n",
        "    failure_count_col: str = \"failure_count\",\n",
        "    pressure_col: str = \"pressure_deviation\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Engineer lag-based features such as rolling failure counts and mean deviation.\"\"\"\n",
        "\n",
        "    if config.timestamp_column not in df.columns:\n",
        "        LOGGER.warning(\n",
        "            \"Timestamp column %s not found; lag features will be skipped.\",\n",
        "            config.timestamp_column,\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    df[config.timestamp_column] = pd.to_datetime(df[config.timestamp_column], errors=\"coerce\")\n",
        "\n",
        "    for lag in config.lag_days:\n",
        "        window = f\"{lag}D\"\n",
        "        group = df.groupby(config.unit_id_column, group_keys=False)\n",
        "\n",
        "        if failure_count_col in df.columns:\n",
        "            df[_lag_feature_name(failure_count_col, lag, \"sum\")] = group[\n",
        "                failure_count_col\n",
        "            ].apply(lambda s: s.rolling(window=window, on=df[config.timestamp_column]).sum())\n",
        "\n",
        "        if pressure_col in df.columns:\n",
        "            df[_lag_feature_name(pressure_col, lag, \"mean\")] = group[pressure_col].apply(\n",
        "                lambda s: s.rolling(window=window, on=df[config.timestamp_column]).mean()\n",
        "            )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def engineer_static_features(\n",
        "    df: pd.DataFrame, config: FailureRiskConfig\n",
        ") -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "    \"\"\"Infer categorical and numerical columns when not provided.\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "    cat_cols = config.categorical_cols or df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    num_cols = config.numerical_cols or df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "\n",
        "    # Remove the target and identifier columns from feature lists\n",
        "    for removable in (config.target_column, config.unit_id_column):\n",
        "        if removable in cat_cols:\n",
        "            cat_cols.remove(removable)\n",
        "        if removable in num_cols:\n",
        "            num_cols.remove(removable)\n",
        "\n",
        "    # Optional textual embedding placeholder: we keep text columns separate to vectorize later\n",
        "    if config.text_column and config.text_column in cat_cols:\n",
        "        cat_cols.remove(config.text_column)\n",
        "\n",
        "    return df, cat_cols, num_cols\n",
        "\n",
        "\n",
        "def build_preprocessor(\n",
        "    categorical_cols: Iterable[str],\n",
        "    numerical_cols: Iterable[str],\n",
        "    text_column: Optional[str],\n",
        "    text_max_features: int,\n",
        ") -> ColumnTransformer:\n",
        "    \"\"\"Create a preprocessing pipeline for categorical, numerical, and text columns.\"\"\"\n",
        "\n",
        "    transformers = []\n",
        "\n",
        "    if numerical_cols:\n",
        "        transformers.append(\n",
        "            (\n",
        "                \"num\",\n",
        "                Pipeline(\n",
        "                    steps=[\n",
        "                        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                        (\"scaler\", StandardScaler()),\n",
        "                    ]\n",
        "                ),\n",
        "                list(numerical_cols),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if categorical_cols:\n",
        "        transformers.append(\n",
        "            (\n",
        "                \"cat\",\n",
        "                Pipeline(\n",
        "                    steps=[\n",
        "                        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                        (\n",
        "                            \"onehot\",\n",
        "                            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
        "                        ),\n",
        "                    ]\n",
        "                ),\n",
        "                list(categorical_cols),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if text_column:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "        def select_text_column(X: pd.DataFrame) -> pd.Series:\n",
        "            return X[text_column].fillna(\"\")\n",
        "\n",
        "        transformers.append(\n",
        "            (\n",
        "                \"text\",\n",
        "                Pipeline(\n",
        "                    steps=[\n",
        "                        (\n",
        "                            \"selector\",\n",
        "                            FunctionTransformer(select_text_column, validate=False),\n",
        "                        ),\n",
        "                        (\n",
        "                            \"tfidf\",\n",
        "                            TfidfVectorizer(\n",
        "                                max_features=text_max_features,\n",
        "                                ngram_range=(1, 2),\n",
        "                                min_df=2,\n",
        "                            ),\n",
        "                        ),\n",
        "                    ]\n",
        "                ),\n",
        "                [text_column],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "\n",
        "\n",
        "def _build_logistic_regression_pipeline(\n",
        "    preprocessor: ColumnTransformer,\n",
        "    class_weight: Optional[Dict[str, float]],\n",
        ") -> Pipeline:\n",
        "    return Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\n",
        "                \"classifier\",\n",
        "                LogisticRegression(\n",
        "                    class_weight=class_weight,\n",
        "                    max_iter=1000,\n",
        "                    solver=\"lbfgs\",\n",
        "                ),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def _build_lightgbm_pipeline(\n",
        "    preprocessor: ColumnTransformer,\n",
        "    random_state: int,\n",
        "    class_weight: Optional[Dict[str, float]],\n",
        ") -> Pipeline:\n",
        "    if lgb is None:\n",
        "        raise RuntimeError(\n",
        "            \"LightGBM is not installed. Please install `lightgbm` to use this model.\"\n",
        "        )\n",
        "\n",
        "    classifier = lgb.LGBMClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=random_state,\n",
        "        class_weight=class_weight,\n",
        "    )\n",
        "\n",
        "    return Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"classifier\", classifier),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def _build_catboost_pipeline(\n",
        "    preprocessor: ColumnTransformer,\n",
        "    random_state: int,\n",
        "    class_weight: Optional[Dict[str, float]],\n",
        ") -> Pipeline:\n",
        "    if CatBoostClassifier is None:\n",
        "        raise RuntimeError(\n",
        "            \"CatBoost is not installed. Please install `catboost` to use this model.\"\n",
        "        )\n",
        "\n",
        "    classifier = CatBoostClassifier(\n",
        "        iterations=800,\n",
        "        learning_rate=0.03,\n",
        "        depth=6,\n",
        "        eval_metric=\"Logloss\",\n",
        "        verbose=False,\n",
        "        random_seed=random_state,\n",
        "        class_weights=class_weight,\n",
        "    )\n",
        "\n",
        "    return Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"classifier\", classifier),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def _compute_class_weight(y: pd.Series) -> Dict[int, float]:\n",
        "    \"\"\"Compute inverse frequency class weights.\"\"\"\n",
        "\n",
        "    value_counts = y.value_counts(normalize=True)\n",
        "    return {cls: 1.0 / freq for cls, freq in value_counts.items()}\n",
        "\n",
        "\n",
        "def resample_with_smote(\n",
        "    X: pd.DataFrame, y: pd.Series, random_state: int\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Apply SMOTE oversampling to handle class imbalance.\"\"\"\n",
        "\n",
        "    LOGGER.info(\"Applying SMOTE oversampling\")\n",
        "    smote = SMOTE(random_state=random_state)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "\n",
        "def calibrate_model(\n",
        "    model: Pipeline,\n",
        "    method: str,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        ") -> CalibratedClassifierCV:\n",
        "    \"\"\"Calibrate probability outputs using Platt scaling or isotonic regression.\"\"\"\n",
        "\n",
        "    LOGGER.info(\"Calibrating model with %s calibration\", method)\n",
        "    calibrator = CalibratedClassifierCV(\n",
        "        base_estimator=model,\n",
        "        method=method,\n",
        "        cv=3,\n",
        "    )\n",
        "    calibrator.fit(X_train, y_train)\n",
        "    return calibrator\n",
        "\n",
        "\n",
        "def train_models(\n",
        "    df: pd.DataFrame,\n",
        "    config: FailureRiskConfig,\n",
        ") -> Dict[str, ClassifierMixin]:\n",
        "    \"\"\"Train baseline and boosted models according to the pipeline specification.\"\"\"\n",
        "\n",
        "    if config.target_column not in df.columns:\n",
        "        raise KeyError(f\"Target column `{config.target_column}` not found in data frame\")\n",
        "\n",
        "    X = df.drop(columns=[config.target_column])\n",
        "    y = df[config.target_column].astype(int)\n",
        "\n",
        "    LOGGER.info(\"Performing train/validation split with train size %.2f\", config.train_size)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, train_size=config.train_size, random_state=config.random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    validation_ratio = config.validation_size / (1 - config.train_size)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "        X_temp,\n",
        "        y_temp,\n",
        "        test_size=1 - validation_ratio,\n",
        "        random_state=config.random_state,\n",
        "        stratify=y_temp,\n",
        "    )\n",
        "\n",
        "    df_train = X_train.assign(**{config.target_column: y_train})\n",
        "\n",
        "    df_train, categorical_cols, numerical_cols = engineer_static_features(df_train, config)\n",
        "    preprocessor = build_preprocessor(\n",
        "        categorical_cols=categorical_cols,\n",
        "        numerical_cols=numerical_cols,\n",
        "        text_column=config.text_column,\n",
        "        text_max_features=config.text_max_features,\n",
        "    )\n",
        "\n",
        "    class_weight = None\n",
        "    if not config.use_smote:\n",
        "        class_weight = _compute_class_weight(y_train)\n",
        "        LOGGER.info(\"Using cost-sensitive class weights: %s\", class_weight)\n",
        "\n",
        "    logistic_pipeline = _build_logistic_regression_pipeline(preprocessor, class_weight)\n",
        "\n",
        "    training_features = df_train.drop(columns=[config.target_column])\n",
        "    training_target = df_train[config.target_column]\n",
        "\n",
        "    if config.use_smote:\n",
        "        training_features, training_target = resample_with_smote(\n",
        "            training_features, training_target, config.random_state\n",
        "        )\n",
        "\n",
        "    LOGGER.info(\"Training logistic regression baseline model\")\n",
        "    logistic_pipeline.fit(training_features, training_target)\n",
        "\n",
        "    models: Dict[str, ClassifierMixin] = {\"logistic_regression\": logistic_pipeline}\n",
        "\n",
        "    try:\n",
        "        LOGGER.info(\"Training LightGBM model\")\n",
        "        lightgbm_pipeline = _build_lightgbm_pipeline(\n",
        "            preprocessor=preprocessor,\n",
        "            random_state=config.random_state,\n",
        "            class_weight=None if config.use_smote else class_weight,\n",
        "        )\n",
        "        lightgbm_pipeline.fit(training_features, training_target)\n",
        "        models[\"lightgbm\"] = lightgbm_pipeline\n",
        "    except RuntimeError as err:\n",
        "        LOGGER.warning(\"LightGBM training skipped: %s\", err)\n",
        "\n",
        "    if \"lightgbm\" not in models and CatBoostClassifier is not None:\n",
        "        LOGGER.info(\"Training CatBoost model as fallback\")\n",
        "        catboost_pipeline = _build_catboost_pipeline(\n",
        "            preprocessor=preprocessor,\n",
        "            random_state=config.random_state,\n",
        "            class_weight=None if config.use_smote else class_weight,\n",
        "        )\n",
        "        catboost_pipeline.fit(training_features, training_target)\n",
        "        models[\"catboost\"] = catboost_pipeline\n",
        "\n",
        "    # Optionally calibrate probability outputs\n",
        "    if config.calibration_method:\n",
        "        for name, model in list(models.items()):\n",
        "            LOGGER.info(\"Calibrating %s model\", name)\n",
        "            models[f\"{name}_calibrated\"] = calibrate_model(\n",
        "                model=model,\n",
        "                method=config.calibration_method,\n",
        "                X_train=X_valid,\n",
        "                y_train=y_valid,\n",
        "            )\n",
        "\n",
        "    LOGGER.info(\"Training complete. Evaluating models on the holdout set.\")\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            y_pred = model.predict(X_test)\n",
        "            LOGGER.info(\"Model %s performance:\\n%s\", name, classification_report(y_test, y_pred))\n",
        "        except NotFittedError:\n",
        "            LOGGER.warning(\"Model %s is not fitted; skipping evaluation.\", name)\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def explain_global(model: ClassifierMixin, X: pd.DataFrame) -> Optional[object]:\n",
        "    \"\"\"Compute global SHAP values for the provided model.\"\"\"\n",
        "\n",
        "    if shap is None:\n",
        "        LOGGER.warning(\"SHAP is not installed; skipping explainability.\")\n",
        "        return None\n",
        "\n",
        "    LOGGER.info(\"Computing SHAP global explanations\")\n",
        "\n",
        "    # Extract the underlying estimator if the model is wrapped in a calibrator or pipeline\n",
        "    estimator = model\n",
        "    if isinstance(model, CalibratedClassifierCV):\n",
        "        estimator = model.base_estimator  # type: ignore[attr-defined]\n",
        "\n",
        "    if isinstance(estimator, Pipeline):\n",
        "        preprocessor = estimator.named_steps[\"preprocessor\"]\n",
        "        classifier = estimator.named_steps[\"classifier\"]\n",
        "        transformed_data = preprocessor.transform(X)\n",
        "    else:\n",
        "        classifier = estimator\n",
        "        transformed_data = X\n",
        "\n",
        "    explainer = shap.Explainer(classifier, transformed_data)\n",
        "    shap_values = explainer(transformed_data)\n",
        "    shap.summary_plot(shap_values, transformed_data, show=False)\n",
        "    return shap_values\n",
        "\n",
        "\n",
        "def explain_local(\n",
        "    model: ClassifierMixin,\n",
        "    X: pd.DataFrame,\n",
        "    instance_indices: Iterable[int],\n",
        ") -> List[object]:\n",
        "    \"\"\"Generate SHAP force plots for specific instances to aid maintenance decisions.\"\"\"\n",
        "\n",
        "    if shap is None:\n",
        "        LOGGER.warning(\"SHAP is not installed; skipping explainability.\")\n",
        "        return []\n",
        "\n",
        "    LOGGER.info(\"Computing SHAP local explanations for instances: %s\", list(instance_indices))\n",
        "\n",
        "    estimator = model\n",
        "    if isinstance(model, CalibratedClassifierCV):\n",
        "        estimator = model.base_estimator  # type: ignore[attr-defined]\n",
        "\n",
        "    if isinstance(estimator, Pipeline):\n",
        "        preprocessor = estimator.named_steps[\"preprocessor\"]\n",
        "        classifier = estimator.named_steps[\"classifier\"]\n",
        "        transformed_data = preprocessor.transform(X)\n",
        "    else:\n",
        "        classifier = estimator\n",
        "        transformed_data = X\n",
        "\n",
        "    explainer = shap.Explainer(classifier, transformed_data)\n",
        "    shap_values = explainer(transformed_data)\n",
        "\n",
        "    force_plots = []\n",
        "    for idx in instance_indices:\n",
        "        force_plot = shap.force_plot(\n",
        "            base_value=shap_values.base_values[idx],\n",
        "            shap_values=shap_values.values[idx],\n",
        "            features=transformed_data[idx],\n",
        "            matplotlib=True,\n",
        "            show=False,\n",
        "        )\n",
        "        force_plots.append(force_plot)\n",
        "\n",
        "    return force_plots\n",
        "\n",
        "\n",
        "def run_training_pipeline(config: FailureRiskConfig) -> Dict[str, ClassifierMixin]:\n",
        "    \"\"\"High-level orchestration for the failure risk pipeline.\"\"\"\n",
        "\n",
        "    LOGGER.info(\"Starting failure risk training pipeline\")\n",
        "    scania_df = load_scania_data(config)\n",
        "    maintnet_df = load_maintnet_data(config)\n",
        "    merged_df = merge_datasets(scania_df, maintnet_df, config)\n",
        "    enriched_df = engineer_lag_features(merged_df, config)\n",
        "    trained_models = train_models(enriched_df, config)\n",
        "    LOGGER.info(\"Pipeline completed successfully\")\n",
        "    return trained_models\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"FailureRiskConfig\",\n",
        "    \"run_training_pipeline\",\n",
        "    \"engineer_lag_features\",\n",
        "    \"train_models\",\n",
        "    \"explain_global\",\n",
        "    \"explain_local\",\n",
        "]"
      ]
    }
  ]
}